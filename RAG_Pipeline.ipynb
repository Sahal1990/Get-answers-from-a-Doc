{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n"
      ],
      "metadata": {
        "id": "3eUnyWFXYzYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL4LS1suTlVx"
      },
      "outputs": [],
      "source": [
        "# 2Ô∏è‚É£ Imports and basic setup\n",
        "\n",
        "import os\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from groq import Groq\n",
        "from pypdf import PdfReader\n",
        "\n",
        "from google.colab import files  # For file upload in Colab\n",
        "import textwrap\n",
        "import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£.1 Set Groq API key (secure input in Colab)\n",
        "\n",
        "# You will be prompted to paste your API key. It will NOT be shown on screen.\n",
        "if \"GROQ_API_KEY\" not in os.environ or not os.environ[\"GROQ_API_KEY\"]:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your GROQ_API_KEY: \")\n",
        "\n",
        "groq_api_key = os.environ[\"GROQ_API_KEY\"]\n",
        "\n",
        "# Create Groq client\n",
        "client = Groq(api_key=groq_api_key)\n",
        "\n",
        "# Choose LLM model (you can change this if you want)\n",
        "GROQ_MODEL = \"openai/gpt-oss-120b\"  # example model name\n"
      ],
      "metadata": {
        "id": "cChw_gIIYW9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî§ 3. Load the embedding model\n",
        "\n",
        "We will use a popular, fast Sentence Transformer:\n",
        "\n",
        "- `sentence-transformers/all-MiniLM-L6-v2`\n",
        "\n"
      ],
      "metadata": {
        "id": "3wby7xO1ZllY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ Load embedding model\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# Get embedding dimension to build FAISS index later\n",
        "EMBED_DIM = embedding_model.get_sentence_embedding_dimension()\n",
        "print(\"Embedding dimension:\", EMBED_DIM)\n"
      ],
      "metadata": {
        "id": "-3r4Sqr3ZIpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ 4. Upload a document (PDF or TXT)\n",
        "\n",
        "You can upload:\n",
        "- A **PDF** file (e.g., book, report)\n",
        "- A **TXT** file (plain text, article, notes)\n",
        "\n",
        "We will extract text from the file for chunking and indexing.\n"
      ],
      "metadata": {
        "id": "DVEpLtMwZ1Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4Ô∏è‚É£ Helper function: load text from uploaded file\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file using pypdf.\"\"\"\n",
        "    reader = PdfReader(file_path)\n",
        "    pages_text = []\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            pages_text.append(page.extract_text() or \"\")\n",
        "        except Exception:\n",
        "            # If extraction fails for a page, skip it\n",
        "            continue\n",
        "    return \"\\n\".join(pages_text)\n",
        "\n",
        "\n",
        "def load_document_text() -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Lets user upload a single file via Colab, returns (file_name, text_content).\n",
        "    Supports .pdf and .txt files.\n",
        "    \"\"\"\n",
        "    uploaded = files.upload()  # Opens a file picker in Colab\n",
        "\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file uploaded.\")\n",
        "\n",
        "    # Take the first uploaded file\n",
        "    file_name = next(iter(uploaded.keys()))\n",
        "    file_path = file_name  # Colab saves it in current working directory\n",
        "\n",
        "    if file_name.lower().endswith(\".pdf\"):\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif file_name.lower().endswith(\".txt\"):\n",
        "        text = uploaded[file_name].decode(\"utf-8\", errors=\"ignore\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Please upload a .pdf or .txt file.\")\n",
        "\n",
        "    # Basic cleaning\n",
        "    text = text.replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
        "    return file_name, text.strip()\n",
        "\n",
        "\n",
        "print(\"‚¨ÜÔ∏è Run the next cell to upload your document.\")\n"
      ],
      "metadata": {
        "id": "lAhm7xUmZip7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì• Upload your document now\n"
      ],
      "metadata": {
        "id": "AqyoZff2aEeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name, raw_text = load_document_text()\n",
        "print(f\"Loaded file: {file_name}\")\n",
        "print(\"Total characters in document:\", len(raw_text))\n",
        "print(\"\\nPreview (first 500 characters):\\n\")\n",
        "print(raw_text[:500])\n"
      ],
      "metadata": {
        "id": "u8hLO5HVZuhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è 5. Dynamic chunking\n",
        "\n",
        "We want the **number of chunks to depend on the document size**:\n",
        "\n",
        "- A small article ‚Üí only a few chunks (e.g., 3‚Äì4)\n",
        "- A big book ‚Üí many chunks (e.g., hundreds)\n",
        "\n",
        "We‚Äôll:\n",
        "\n",
        "1. Decide how many chunks we *roughly* want based on total characters  \n",
        "2. Derive a chunk size from that  \n",
        "3. Split the text accordingly with some overlap for continuity\n"
      ],
      "metadata": {
        "id": "ed5gergLaVhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Dynamic chunking utilities\n",
        "\n",
        "def compute_dynamic_chunk_size(\n",
        "    total_chars: int,\n",
        "    base_target_chars_per_chunk: int = 900,\n",
        "    min_chunks: int = 3,\n",
        "    max_chunks: int = 400,\n",
        ") -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Decide dynamic chunk size based on document length.\n",
        "\n",
        "    - `base_target_chars_per_chunk`: ideal chunk size if the doc is large\n",
        "    - Number of chunks is bounded between min_chunks and max_chunks\n",
        "\n",
        "    Returns: (chunk_size, estimated_num_chunks)\n",
        "    \"\"\"\n",
        "    if total_chars <= 0:\n",
        "        raise ValueError(\"Document is empty.\")\n",
        "\n",
        "    # For large documents, chunks ~= total_chars / base_target_chars_per_chunk\n",
        "    approx_chunks = math.ceil(total_chars / base_target_chars_per_chunk)\n",
        "\n",
        "    # Clip between min and max\n",
        "    approx_chunks = max(min_chunks, min(max_chunks, approx_chunks))\n",
        "\n",
        "    # Derive chunk size to get about that many chunks\n",
        "    chunk_size = math.ceil(total_chars / approx_chunks)\n",
        "\n",
        "    return chunk_size, approx_chunks\n",
        "\n",
        "\n",
        "def chunk_text(\n",
        "    text: str,\n",
        "    chunk_size: int,\n",
        "    chunk_overlap: int = 150\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "\n",
        "    - chunk_size: max characters per chunk\n",
        "    - chunk_overlap: characters overlapped between consecutive chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text)\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk.strip())\n",
        "\n",
        "        # Move start forward but keep some overlap\n",
        "        start = end - chunk_overlap\n",
        "\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "\n",
        "    # Remove any empty chunks\n",
        "    chunks = [c for c in chunks if c]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "IvChIVw7aKD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£.1 Apply dynamic chunking to your document\n",
        "\n",
        "total_chars = len(raw_text)\n",
        "\n",
        "dynamic_chunk_size, estimated = compute_dynamic_chunk_size(\n",
        "    total_chars,\n",
        "    base_target_chars_per_chunk=900,  # You can tweak this\n",
        "    min_chunks=3,\n",
        "    max_chunks=500,\n",
        ")\n",
        "\n",
        "print(f\"Estimated number of chunks: ~{estimated}\")\n",
        "print(f\"Dynamic chunk size: {dynamic_chunk_size} characters\\n\")\n",
        "\n",
        "# Now actually create chunks\n",
        "chunks = chunk_text(raw_text, chunk_size=dynamic_chunk_size, chunk_overlap=150)\n",
        "\n",
        "print(f\"Actual number of chunks created: {len(chunks)}\")\n",
        "print(\"\\nExample chunk (first one):\\n\")\n",
        "print(textwrap.fill(chunks[0][:600], width=100))\n"
      ],
      "metadata": {
        "id": "A7XCKphZagIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß¨ 6. Build FAISS index from chunks\n",
        "\n",
        "Steps:\n",
        "1. Create embeddings for each chunk using Sentence Transformers  \n",
        "2. Store them in a FAISS index for **fast similarity search**  \n",
        "3. Keep a mapping from index ‚Üí original chunk text\n"
      ],
      "metadata": {
        "id": "aY5zDp1JatuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6Ô∏è‚É£ Create embeddings and FAISS index\n",
        "\n",
        "def build_faiss_index(chunks: List[str]):\n",
        "    \"\"\"\n",
        "    Takes a list of text chunks, returns:\n",
        "      - FAISS index\n",
        "      - embeddings (as a numpy array)\n",
        "      - chunk_texts list (same order as embeddings)\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        raise ValueError(\"No chunks to index.\")\n",
        "\n",
        "    # Compute embeddings (batch)\n",
        "    embeddings = embedding_model.encode(\n",
        "        chunks,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    # Normalize embeddings for cosine similarity with inner product\n",
        "    # FAISS IndexFlatIP assumes we're using inner product; normalizing\n",
        "    # makes inner product equivalent to cosine similarity.\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    index = faiss.IndexFlatIP(EMBED_DIM)  # IP = inner product\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return index, embeddings, chunks\n",
        "\n",
        "\n",
        "faiss_index, doc_embeddings, doc_chunks = build_faiss_index(chunks)\n",
        "print(\"FAISS index built with\", len(doc_chunks), \"chunks.\")\n"
      ],
      "metadata": {
        "id": "3lLzOhH3apD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç 7. Define a RAG query function\n",
        "\n",
        "For each user question:\n",
        "\n",
        "1. Convert the question into an embedding  \n",
        "2. Use FAISS to find the **top-k most similar chunks**  \n",
        "3. Pass those chunks + question as context to Groq LLM  \n",
        "4. Get an answer grounded in the document\n"
      ],
      "metadata": {
        "id": "NCl9SLC8a7Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7Ô∏è‚É£ RAG query function\n",
        "\n",
        "def retrieve_relevant_chunks(\n",
        "    query: str,\n",
        "    index,\n",
        "    k: int = 4\n",
        ") -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar chunks to the query.\n",
        "    Returns list of (chunk_text, score).\n",
        "    \"\"\"\n",
        "    # Encode query\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Search in FAISS\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(indices[0], distances[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        results.append((doc_chunks[idx], float(score)))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def build_context_from_chunks(chunks_with_scores: List[Tuple[str, float]]) -> str:\n",
        "    \"\"\"\n",
        "    Concatenate top chunks into a single context string.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for i, (chunk, score) in enumerate(chunks_with_scores, start=1):\n",
        "        header = f\"\\n\\n[Chunk {i} | score={score:.3f}]\\n\"\n",
        "        parts.append(header + chunk)\n",
        "    return \"\".join(parts)\n",
        "\n",
        "\n",
        "def ask_groq_llm(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Send the question + context to Groq LLM and get an answer.\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant using the provided context from a document. \"\n",
        "        \"Answer the question ONLY using the context. If the answer is not in the \"\n",
        "        \"context, say you don't know.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Context:\\n{context}\\n\\n\"\n",
        "                f\"Question: {question}\\n\\n\"\n",
        "                \"Answer based only on the above context.\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=GROQ_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def rag_answer(question: str, top_k: int = 4, show_context: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    High-level helper:\n",
        "    1. Retrieve relevant chunks\n",
        "    2. Build context\n",
        "    3. Ask Groq LLM\n",
        "    4. Print results\n",
        "    \"\"\"\n",
        "    print(f\"üîé Question: {question}\\n\")\n",
        "\n",
        "    # 1. Retrieve\n",
        "    top_chunks = retrieve_relevant_chunks(question, faiss_index, k=top_k)\n",
        "\n",
        "    # 2. Build context\n",
        "    context = build_context_from_chunks(top_chunks)\n",
        "\n",
        "    if show_context:\n",
        "        print(\"üìö Retrieved context (truncated):\\n\")\n",
        "        print(textwrap.shorten(context, width=1200, placeholder=\"...\"))\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # 3. Ask LLM\n",
        "    answer = ask_groq_llm(question, context)\n",
        "\n",
        "    print(\"üß† Answer:\\n\")\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "id": "_UZyIc1ia3KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ 8. Try asking questions!\n",
        "\n",
        "Now you can ask questions about the uploaded document.  \n",
        "The system will:\n",
        "\n",
        "- Dynamically chunk the document  \n",
        "- Retrieve the most relevant chunks  \n",
        "- Use Groq LLM to generate an answer based **only** on the document\n"
      ],
      "metadata": {
        "id": "PhSERnZgbKNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£ Ask your first question\n",
        "\n",
        "rag_answer(\"Iss insaan ko December me kitne paise mile?\", top_k=4, show_context=False)\n"
      ],
      "metadata": {
        "id": "K3JGcl4obGTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£.1 Ask more questions (you can edit this cell and re-run)\n",
        "\n",
        "rag_answer(\"Summarize the key ideas from the document.\", top_k=5, show_context=False)\n"
      ],
      "metadata": {
        "id": "ZxN8tg7bbNv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YK31HGYTcLtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}